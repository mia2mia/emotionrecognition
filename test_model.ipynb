{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import speech_recognition as sr\n",
    "# from keras.layers import GlobalAveragePooling1D\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "from sklearn.utils import class_weight as clw\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "#TODO: load the correct feature extraction function\n",
    "from audio_features import extract_logmel as extract_feat\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MeanPool(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MeanPool, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.input_spec = InputSpec(ndim=3)\n",
    "\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "      # do not pass the mask to the next layers\n",
    "      return None\n",
    "\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if mask is not None:\n",
    "            # mask (batch, time)\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            # mask (batch, x_dim, time)\n",
    "            mask = K.repeat(mask, x.shape[-1])\n",
    "            # mask (batch, time, x_dim)\n",
    "            mask = K.permute_dimensions(mask, (0,2,1))\n",
    "            x = x * mask\n",
    "        return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
    "\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # remove temporal dimension\n",
    "        return (input_shape[0], input_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class emoLSTM():\n",
    "    \n",
    "    def __init__(self, pre_trained):\n",
    "        self.model = load_model(pre_trained, custom_objects={'MeanPool': MeanPool})\n",
    "\n",
    "\n",
    "    def evaluate(self, x_test, y_test, sample_weight=None):\n",
    "        score = self.model.evaluate(x=pad_sequences(x_test), y=y_test, sample_weight=sample_weight)\n",
    "        # print (\"Test Loss: {}, Test UA: {}, Test WA: {}\".format(score[0], score[1], score[2]))\n",
    "        print (score)\n",
    "        print (self.model.metrics_names)\n",
    "        \n",
    "\n",
    "    def predict(self, x_test):\n",
    "        return self.model.predict(pad_sequences(x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AudioRec(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.r = sr.Recognizer()\n",
    "        self.src = sr.Microphone()\n",
    "        with self.src as source:\n",
    "            print(\"Calibrating microphone...\")\n",
    "            self.r.adjust_for_ambient_noise(source, duration=2)\n",
    "\n",
    "\n",
    "    def listen(self, save_path):\n",
    "        with self.src as source:\n",
    "            print(\"Recording ...\")\n",
    "            # record for a maximum of 10s\n",
    "            audio = self.r.listen(source, phrase_time_limit=10)\n",
    "        # write audio to a WAV file\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            f.write(audio.get_wav_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(mini_batch):\n",
    "    batch = np.copy(mini_batch)\n",
    "    max_len = max([example.shape[0] for example in batch])\n",
    "    feat_len = batch[0].shape[1]\n",
    "    for i,example in enumerate(batch):\n",
    "        seq_len = example.shape[0]\n",
    "        if seq_len != max_len:\n",
    "            batch[i] = np.vstack([example, np.zeros((max_len - seq_len, feat_len), dtype=example.dtype)])\n",
    "    return np.dstack(batch).transpose(2,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = 'models/emorec_model_0703_035905.val-acc-0.5656.h5'\n",
    "realtime_mode = True\n",
    "wav_path = 'microphone-results.wav'\n",
    "\n",
    "# previously tried 6 emotions\n",
    "# labels = {0:'ang', 1:'hap', 2:'exc', 3:'sad', 4:'fru', 5:'neu'}\n",
    "labels = {0:'ang', 1:'hap', 2:'sad', 3:'neu'}\n",
    "\n",
    "enn = emoLSTM(weights_path)\n",
    "enn.model.summary()\n",
    "\n",
    "if realtime_mode:\n",
    "    ar = AudioRec()\n",
    "    while True:\n",
    "        try:\n",
    "            ar.listen(\"microphone-results.wav\")\n",
    "            features = extract_feat(\"microphone-results.wav\")\n",
    "\n",
    "            if features is not None:\n",
    "                print (\"Extracted features of shape: \", features.shape)\n",
    "                y_pred = enn.predict(features[None,:])\n",
    "                pred_class = labels[np.argmax(y_pred, axis=1)[0]]\n",
    "                print (\"Predicted emotion is: \", pred_class)\n",
    "            time.sleep(0.1)\n",
    "        except KeyboardInterrupt:\n",
    "            print (\"Quitting realtime application..\")\n",
    "            break\n",
    "\n",
    "else:\n",
    "    wav_path = args[\"audio\"]\n",
    "    features = extract_feat(wav_path)\n",
    "    print (\"Extracted features of shape: \", features.shape)\n",
    "    y_pred = enn.predict(features[None,:])\n",
    "    pred_class = labels[np.argmax(y_pred, axis=1)[0]]\n",
    "    print(\"Predicted emotion is: \", pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
